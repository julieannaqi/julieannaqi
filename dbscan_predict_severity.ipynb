{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE/aZHBFz9nrPV69hLEl2p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julieannaqi/julieannaqi/blob/main/dbscan_predict_severity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrVNFUi1oG4g",
        "outputId": "26d645ef-75ff-4108-ed0b-141b9d318849"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmwqsw7M_kd-",
        "outputId": "4edbf313-786a-45a5-baaa-133cd2fb900a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.10/dist-packages (0.61.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory_profiler) (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'juile.csv' with the actual filename if it's different\n",
        "df = pd.read_csv('/content/sampledata.csv')\n",
        "\n",
        "df = df.head(500)"
      ],
      "metadata": {
        "id": "eUP_Vsahn0TQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwaQVj909QnM",
        "outputId": "08e21891-944f-463b-e652-1bfc9ff28745"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ioc_match_id', 'event_time', 'src_host', 'indicator', 'confidence',\n",
              "       'severity', 'indicator_type', 'originator', 'source_feed_id',\n",
              "       'event.action', 'event.dest', 'event.dest_ip', 'event.src',\n",
              "       'event.src_ip', 'count', 'event.src_port', 'event.dest_port',\n",
              "       'timestamp', 'event.sourcetype', 'source', 'itype', 'hour', 'minute'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZr5OBUt9hFp",
        "outputId": "92b1cf28-b3b8-4bec-a9ad-bb2f84ea3f40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.corr of                           ioc_match_id     event_time       src_host  \\\n",
              "0      1773_362000776412_0_60382090314  1701618689736  10.15.149.168   \n",
              "1      1773_361424542980_0_59299513186  1701589169373  10.15.149.168   \n",
              "2      1773_361424542980_0_59327455359  1701589169373  10.15.149.168   \n",
              "3      1773_361424542980_0_59327639236  1701589169373  10.15.149.168   \n",
              "4      1773_362000776412_0_60440942415  1701618689736  10.15.149.168   \n",
              "...                                ...            ...            ...   \n",
              "99995  1773_361569283028_0_60382090483  1701596405801  10.20.100.250   \n",
              "99996  1773_361569283028_0_60724428162  1701596405801  10.20.100.250   \n",
              "99997  1773_361569283121_0_60362960162  1701596405801   10.20.106.39   \n",
              "99998  1773_361569283121_0_60374602795  1701596405801   10.20.106.39   \n",
              "99999  1773_361569283121_0_60650052021  1701596405801   10.20.106.39   \n",
              "\n",
              "            indicator  confidence severity indicator_type  originator  \\\n",
              "0        77.90.185.71          87      low             ip           0   \n",
              "1      198.235.24.186          99   medium             ip           0   \n",
              "2      198.235.24.186         100      low             ip           0   \n",
              "3      198.235.24.186         100      low             ip           0   \n",
              "4        77.90.185.71          87   medium             ip           0   \n",
              "...               ...         ...      ...            ...         ...   \n",
              "99995   77.90.185.240          87      low             ip           0   \n",
              "99996   77.90.185.240          87   medium             ip           0   \n",
              "99997   45.129.14.236          95      low             ip           0   \n",
              "99998   45.129.14.236          95      low             ip           0   \n",
              "99999   45.129.14.236          94   medium             ip           0   \n",
              "\n",
              "       source_feed_id event.action  ... event.src_ip count event.src_port  \\\n",
              "0                 260       ACCEPT  ...          NaN     1          47250   \n",
              "1                 469       ACCEPT  ...          NaN     1          56140   \n",
              "2                 280       ACCEPT  ...          NaN     1          56140   \n",
              "3                 156       ACCEPT  ...          NaN     1          56140   \n",
              "4                 469       ACCEPT  ...          NaN     1          47250   \n",
              "...               ...          ...  ...          ...   ...            ...   \n",
              "99995             260       ACCEPT  ...          NaN     1          52780   \n",
              "99996             469       ACCEPT  ...          NaN     1          52780   \n",
              "99997             260       ACCEPT  ...          NaN     1          44925   \n",
              "99998              18       ACCEPT  ...          NaN     1          44925   \n",
              "99999             469       ACCEPT  ...          NaN     1          44925   \n",
              "\n",
              "      event.dest_port      timestamp     event.sourcetype  \\\n",
              "0                8080  1701618689736  ULink: AWS VPC Flow   \n",
              "1               32400  1701589169373  ULink: AWS VPC Flow   \n",
              "2               32400  1701589169373  ULink: AWS VPC Flow   \n",
              "3               32400  1701589169373  ULink: AWS VPC Flow   \n",
              "4                8080  1701618689736  ULink: AWS VPC Flow   \n",
              "...               ...            ...                  ...   \n",
              "99995           10000  1701596405801  ULink: AWS VPC Flow   \n",
              "99996           10000  1701596405801  ULink: AWS VPC Flow   \n",
              "99997           56107  1701596405801  ULink: AWS VPC Flow   \n",
              "99998           56107  1701596405801  ULink: AWS VPC Flow   \n",
              "99999           56107  1701596405801  ULink: AWS VPC Flow   \n",
              "\n",
              "                            source     itype hour minute  \n",
              "0      Spamhaus Extended Drop List   spam_ip   15   50-0  \n",
              "1             DShield Scanning IPs   scan_ip    7   30-0  \n",
              "2                   VoIP Blacklist    bot_ip    7   30-0  \n",
              "3       VoIP Blacklist by ScopServ  brute_ip    7   30-0  \n",
              "4             DShield Scanning IPs   scan_ip   15   50-0  \n",
              "...                            ...       ...  ...    ...  \n",
              "99995  Spamhaus Extended Drop List   spam_ip    9   40-0  \n",
              "99996         DShield Scanning IPs   scan_ip    9   40-0  \n",
              "99997  Spamhaus Extended Drop List   spam_ip    9   40-0  \n",
              "99998                      CI Army    bot_ip    9   40-0  \n",
              "99999         DShield Scanning IPs   scan_ip    9   40-0  \n",
              "\n",
              "[100000 rows x 23 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# looks like the time related fields are heavily correlated. Let's only keep the event_Time.\n",
        "df.drop(columns=['timestamp', 'hour'], inplace=True)"
      ],
      "metadata": {
        "id": "NzkrP0fs9vei"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXbUP6jiFDGb",
        "outputId": "da76df0b-363e-43f9-b3a6-2583fa744731"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['ioc_match_id', 'event_time', 'src_host', 'indicator', 'confidence',\n",
            "       'severity', 'indicator_type', 'originator', 'source_feed_id',\n",
            "       'event.action', 'event.dest', 'event.dest_ip', 'event.src',\n",
            "       'event.src_ip', 'count', 'event.src_port', 'event.dest_port',\n",
            "       'event.sourcetype', 'source', 'itype', 'minute'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I don't know much about the data, but let's pretend we want to build a model to predict the severity\n",
        "\n",
        "y = df['severity']\n",
        "predictive_columns = df.columns[df.columns != 'severity']\n",
        "x = df[predictive_columns]"
      ],
      "metadata": {
        "id": "S7LZJD8Q-c9j"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3vjLqX2LxeK",
        "outputId": "7a681f03-6bcb-4dca-b132-26de852fc00c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0           low\n",
            "1        medium\n",
            "2           low\n",
            "3           low\n",
            "4        medium\n",
            "          ...  \n",
            "99995       low\n",
            "99996    medium\n",
            "99997       low\n",
            "99998       low\n",
            "99999    medium\n",
            "Name: severity, Length: 100000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AnABN5NanGIM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
        "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n",
        "\n",
        "def calculate_metrics(data, true_labels, predicted_labels):\n",
        "         homogeneity = homogeneity_score(true_labels, predicted_labels)\n",
        "         completeness = completeness_score(true_labels, predicted_labels)\n",
        "         v_measure = v_measure_score(true_labels, predicted_labels)\n",
        "         adj_rand_index = adjusted_rand_score(true_labels, predicted_labels)\n",
        "         adj_mutual_info = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
        "         silhouette_coeff = silhouette_score(data, predicted_labels)\n",
        "\n",
        "         return {\n",
        "             \"Homogeneity\": homogeneity,\n",
        "             \"Completeness\": completeness,\n",
        "             \"V-Measure\": v_measure,\n",
        "             \"Adjusted Rand Index\": adj_rand_index,\n",
        "             \"Adjusted Mutual Information\": adj_mutual_info,\n",
        "             \"Silhouette Coefficient\": silhouette_coeff\n",
        "         }\n",
        "        #return silhouette_coeff\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Assuming 'severity' is the target variable and the rest are predictors\n",
        "y = df['severity']\n",
        "predictive_columns = df.columns[df.columns != 'severity']\n",
        "X = df[predictive_columns]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify non-numeric and numeric columns\n",
        "non_numeric_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n",
        "numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# ColumnTransformer to handle different transformations for different columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'), non_numeric_cols),\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value=-1), numeric_cols),\n",
        "        ('scaler', StandardScaler(), numeric_cols),\n",
        "        ('to_sparse', FunctionTransformer(csr_matrix), numeric_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# DBSCAN model\n",
        "dbscan = DBSCAN(eps=0.1, min_samples=10)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('dbscan', dbscan)  # Add the DBSCAN model to the pipeline\n",
        "])\n",
        "\n",
        "# Fit the pipeline to your training data\n",
        "pipeline.fit(X_train)\n",
        "\n",
        "true_labels_train = y_train\n",
        "true_labels_test = y_test\n",
        "\n",
        "X_train_processed = pipeline.named_steps['preprocessor'].fit_transform(X_train)\n",
        "\n",
        "# Transform the testing set using the fitted pipeline\n",
        "X_test_processed = pipeline.named_steps['preprocessor'].transform(X_test)\n",
        "\n",
        "# Fit and predict labels on the training set\n",
        "labels_train = pipeline.named_steps['dbscan'].fit_predict(X_train_processed)\n",
        "\n",
        "# Predict labels on the testing set using the trained pipeline\n",
        "labels_test = pipeline.named_steps['dbscan'].fit_predict(X_test_processed)"
      ],
      "metadata": {
        "id": "xdzYkW2u3h8g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics for the training set\n",
        "metrics_train = calculate_metrics(X_train_processed, true_labels_train, labels_train)\n",
        "\n",
        "# Calculate metrics for the testing set\n",
        "metrics_test = calculate_metrics(X_test_processed, true_labels_test, labels_test)\n",
        "\n",
        "# Display or use the metrics as needed\n",
        "print(\"Training Set Metrics:\")\n",
        "print(metrics_train)\n",
        "\n",
        "print(\"\\nTesting Set Metrics:\")\n",
        "print(metrics_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmEqGyaUDfvV",
        "outputId": "1231d3a6-304d-42a4-d0f2-83a720187c79"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Metrics:\n",
            "{'Homogeneity': 0.01360916975476778, 'Completeness': 0.009197286626300644, 'V-Measure': 0.010976491296077199, 'Adjusted Rand Index': -0.00016501499174680458, 'Adjusted Mutual Information': 0.0029869827525054204, 'Silhouette Coefficient': 0.9525522606789859}\n",
            "\n",
            "Testing Set Metrics:\n",
            "{'Homogeneity': 0.01536177620835911, 'Completeness': 0.009603705170100984, 'V-Measure': 0.011818716199194917, 'Adjusted Rand Index': -0.008877917481467162, 'Adjusted Mutual Information': -0.019985575453091604, 'Silhouette Coefficient': 0.7786547855366028}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prettytable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n_xXtPwScsl",
        "outputId": "2091f31f-efcf-43b2-b91e-4e9c5abe87c5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# Display or use the metrics as needed\n",
        "def display_metrics(metrics, set_name):\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Metric\", \"Value\"]\n",
        "\n",
        "    for metric, value in metrics.items():\n",
        "        table.add_row([metric, value])\n",
        "\n",
        "    print(f\"{set_name} Metrics:\")\n",
        "    print(table)\n",
        "    print()\n",
        "\n",
        "# Assuming you have metrics_train and metrics_test dictionaries\n",
        "display_metrics(metrics_train, \"Training Set\")\n",
        "display_metrics(metrics_test, \"Testing Set\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc_Hmvj1SfGq",
        "outputId": "3541b8eb-9489-48d6-8f78-f7f616fcdf16"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Metrics:\n",
            "+-----------------------------+-------------------------+\n",
            "|            Metric           |          Value          |\n",
            "+-----------------------------+-------------------------+\n",
            "|         Homogeneity         |   0.01360916975476778   |\n",
            "|         Completeness        |   0.009197286626300644  |\n",
            "|          V-Measure          |   0.010976491296077199  |\n",
            "|     Adjusted Rand Index     | -0.00016501499174680458 |\n",
            "| Adjusted Mutual Information |  0.0029869827525054204  |\n",
            "|    Silhouette Coefficient   |    0.9525522606789859   |\n",
            "+-----------------------------+-------------------------+\n",
            "\n",
            "Testing Set Metrics:\n",
            "+-----------------------------+-----------------------+\n",
            "|            Metric           |         Value         |\n",
            "+-----------------------------+-----------------------+\n",
            "|         Homogeneity         |  0.01536177620835911  |\n",
            "|         Completeness        |  0.009603705170100984 |\n",
            "|          V-Measure          |  0.011818716199194917 |\n",
            "|     Adjusted Rand Index     | -0.008877917481467162 |\n",
            "| Adjusted Mutual Information | -0.019985575453091604 |\n",
            "|    Silhouette Coefficient   |   0.7786547855366028  |\n",
            "+-----------------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon first impression, the silhouette coefficient tells us that our model produces decently well-formed clusters in both the train and test set, and doesn't appear to be overfitting."
      ],
      "metadata": {
        "id": "5PhCLptlSlri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, from both homogeneity and completeness, we cannot assume that each cluster contains members of a single class. This is ok. It just means that combinations of different columns is more descriptive than a single column."
      ],
      "metadata": {
        "id": "nS3TuQUbTgNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rand index and adjusted mutual information suggests that the true and predicted clusterings are largley dissimilar. We cannot infer much from one about the other."
      ],
      "metadata": {
        "id": "6KrvRID1VgeB"
      }
    }
  ]
}